{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script compares embedded radio transcripts and fact-checks,\n",
    "# returning a CSV file of potential misinformation\n",
    "\n",
    "# ensure libretranslate is running on port 5000 before running this script\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "from libretranslatepy import LibreTranslateAPI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "number_return_values = 1\n",
    "\n",
    "# add your OpenAI API key\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"sk-proj-HSrHGyOHtEDtr9loLzRjT3BlbkFJTUFaqCLYDEs3B7qpBz7z\"))\n",
    "\n",
    "embedded_transcripts_path= Path().cwd().parent.joinpath('data/embedded_transcripts/embedded_transcripts.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Importing embedded transcripts...')\n",
    "\n",
    "transcript_df = pd.read_csv(embedded_transcripts_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input = [text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Importing false statements with embeddings...\")\n",
    "\n",
    "embedded_false_statements_path = Path().cwd().parent.joinpath('data/factchecked_statements/embedded_false_statements.csv')\n",
    "\n",
    "false_text_df = pd.read_csv(embedded_false_statements_path)\n",
    "\n",
    "false_text_df['statement_embedding'] = false_text_df['statement_embedding'].apply(eval).apply(np.array)\n",
    "\n",
    "false_text_df['statement'] = false_text_df['statement'].replace('\\'', '').replace('\\\"', '').replace('\\\"', '').replace('.','').replace('?','').replace('!','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    # Normalize each vector to unit length\n",
    "    vec1_norm = vec1 / np.linalg.norm(vec1)\n",
    "    vec2_norm = vec2 / np.linalg.norm(vec2)\n",
    "  \n",
    "    # Calculate dot product between normalized vectors\n",
    "    similarity = np.dot(vec1_norm, vec2_norm)\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if the transcript segment is shorter than 150 characters, this will use the context segment instead\n",
    "def search_false_statements(search_terms,false_text_df=false_text_df, n=1):\n",
    "    transcription_embedding = search_terms.transcription_embedding\n",
    "    context_embedding = search_terms.transcription_with_context_embedding\n",
    "    if len(transcription_embedding) > 150:\n",
    "        false_text_df['similarities'] = false_text_df['statement_embedding'].apply(lambda x: cosine_similarity(x, transcription_embedding))\n",
    "    else:\n",
    "        false_text_df['similarities'] = false_text_df['statement_embedding'].apply(lambda x: cosine_similarity(x, context_embedding))\n",
    "    results = (\n",
    "        false_text_df.sort_values(\"similarities\", ascending=False).head(n)\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the transcript segment is shorter than 200 characters, this will use the context segment instead\n",
    "def search_false_statements(search_terms,false_text_df=false_text_df, n=1):\n",
    "    transcription_embedding = search_terms.transcription_embedding\n",
    "    context_embedding = search_terms.transcription_with_context_embedding\n",
    "    if len(transcription_embedding) > 200:\n",
    "        false_text_df['similarities'] = false_text_df['statement_embedding'].apply(lambda x: cosine_similarity(x, transcription_embedding))\n",
    "    else:\n",
    "        false_text_df['similarities'] = false_text_df['statement_embedding'].apply(lambda x: cosine_similarity(x, context_embedding))\n",
    "    results = (\n",
    "        false_text_df.sort_values(\"similarities\", ascending=False).head(n)\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_matches_json = []\n",
    "\n",
    "def search_all_transcripts():\n",
    "    errors = 0\n",
    "    for row in transcript_df.itertuples(name='segment'):\n",
    "        match_index = row.Index\n",
    "        try:\n",
    "            results = search_false_statements(row)\n",
    "            top_match = results.statement.values\n",
    "            similarity = results.similarities\n",
    "            top_matches = {}\n",
    "            top_matches[\"index\"] = f\"{match_index}\"\n",
    "            top_matches[\"filename\"] = row.file_name\n",
    "            top_matches[\"input_statement\"] = row.transcription\n",
    "            top_matches[\"checked_false_statement\"] = f\"{top_match}\"\n",
    "            top_matches[\"similarity\"] = f\"{similarity}\"\n",
    "            top_matches_json.append(top_matches)\n",
    "            print_text = f\"Finished checking {match_index + 1} of {len(transcript_df)} statements -- {int((match_index + 1)/len(exploded_transcript_df)*100)}% complete         \"\n",
    "            print(\"\\r\", print_text, end=\"\")\n",
    "        except:\n",
    "            errors += 1\n",
    "            print(f\"Error checking statement {match_index + 1}\")\n",
    "    print(f\"Finished checking all statements with {errors} error(s)\")\n",
    "\n",
    "print(\"Starting search of all transcripts...\")\n",
    "\n",
    "search_all_transcripts()\n",
    "\n",
    "print(\"Creating DataFrame...\")\n",
    "\n",
    "top_matches_df = pd.DataFrame(top_matches_json)\n",
    "\n",
    "top_matches_df['similarity_value'] = top_matches_df['similarity'].apply(lambda x: x.split()[1]).astype(float)\n",
    "top_matches_df['factcheck_index'] = top_matches_df['similarity'].apply(lambda x: x.split()[0])\n",
    "\n",
    "over_50 = top_matches_df.loc[top_matches_df['similarity_value'] >= .5]\n",
    "\n",
    "over_50 = over_50.sort_values('similarity_value', ascending=False)\n",
    "\n",
    "over_50 = over_50[['filename', 'input_statement', 'checked_false_statement', 'similarity_value', 'factcheck_index']]\n",
    "\n",
    "over_50_csv_filepath = Path().cwd().parent.joinpath('data/output_csv/potential_misinformation.csv')\n",
    "\n",
    "print(\"Generating csv file...\")\n",
    "\n",
    "over_50.to_csv(f\"{over_50_csv_filepath}\")\n",
    "\n",
    "\n",
    "print(\"Adding translation...\")\n",
    "\n",
    "lt = LibreTranslateAPI(\"http://localhost:5000\")\n",
    "\n",
    "def libretranslate_spanish(input_text):\n",
    "    return lt.translate(f\"{input_text}\", \"es\", \"en\")\n",
    "\n",
    "def libretranslate_french(input_text):\n",
    "    return lt.translate(f\"{input_text}\", \"fr\", \"en\")\n",
    "\n",
    "try:\n",
    "    over_50['translation'] = over_50['input_statement'].apply(lambda x: libretranslate_french(x))\n",
    "    over_50['translation'] = over_50['translation'].apply(lambda x: libretranslate_spanish(x))\n",
    "\n",
    "    over_50 = over_50.sort_values('similarity_value', ascending=False)\n",
    "\n",
    "    over_50 = over_50[['filename', 'input_statement', 'translation', 'checked_false_statement', 'similarity_value', 'factcheck_index']]\n",
    "\n",
    "    print(\"Generating csv file...\")\n",
    "\n",
    "    translation_filepath = Path().cwd().parent.joinpath('data/output_csv/potential_misinformation_with_translations.csv')\n",
    "\n",
    "    over_50.to_csv(f\"{translation_filepath}\")\n",
    "\n",
    "except:\n",
    "    print(\"Failed to add transcription. Check that LibreTranslate is running.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "misinforadio-testing-VjISiCkA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
